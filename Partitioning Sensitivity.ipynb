{
 "metadata": {
  "name": "",
  "signature": "sha256:7b7ca3f9955f7416ad65b110fb237b5d250023a07c12b2a71b2897fecd0bb7c1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from IPython import parallel as p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cp = p.Client()\n",
      "cp.block=True\n",
      "print cp.ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1, 2]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "view = cp.load_balanced_view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import statsmodels.api as sm\n",
      "fluxes_columns = ['y','m','d','ET','R','BF','sm1','sm2','sm3','SWE','Cs','Qs','Ql','Qg','NR','PEText','PETtrc','PETsrc']\n",
      "forcing_columns = ['P','Tmax','Tmin','W']\n",
      "cols = ['ablationslope','peakswe','ablationseasonP','gsET','wyP', 'ablationseason']\n",
      "dates = pd.read_pickle('/Volumes/Users/Theo/projects/Budyko_vic/timecode.pcl')\n",
      "wyears = np.load('/Volumes/Users/Theo/projects/Budyko_vic/wyears.npy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import statsmodels.api as sm\n",
      "fluxes_columns = ['y','m','d','ET','R','BF','sm1','sm2','sm3','SWE','Cs','Qs','Ql','Qg','NR','PEText','PETtrc','PETsrc']\n",
      "forcing_columns = ['P','Tmax','Tmin','W']\n",
      "cols = ['ablationslope','peakswe','ablationseasonP','gsET','wyP', 'ablationseason']\n",
      "dates = pd.read_pickle('/Volumes/Users/Theo/projects/Budyko_vic/timecode.pcl')\n",
      "wyears = np.load('/Volumes/Users/Theo/projects/Budyko_vic/wyears.npy')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def partitioning(fl,fr):\n",
      "    lat = float(fl.split('_')[-2]) # extract latitude from file name\n",
      "    lon = float(fl.split('_')[-1]) # extract longitude from file name\n",
      "     \n",
      "    # preallocate a matrix for the results\n",
      "    data = pd.DataFrame(columns=cols, index=wyears)\n",
      "    \n",
      "    # load the data\n",
      "    flux = pd.read_table(fl, sep='\\t', names=fluxes_columns) # load fluxes\n",
      "    flux.index = pd.DatetimeIndex(dates)\n",
      "    \n",
      "    forc = pd.read_table(fr, sep=' ', names=forcing_columns) # load forcing\n",
      "    forc.index = pd.DatetimeIndex(dates)\n",
      "    \n",
      "    for wy in wyears:\n",
      "        strt = str(wy-1)+'-10-01'\n",
      "        nd = str(wy)+'-09-30'\n",
      "\n",
      "        data.loc[wy,'peakswe'] = flux[strt:nd].SWE.max() # pull max swe\n",
      "        dopeakswe = flux[strt:nd].SWE.idxmax() # pull day of peak swe\n",
      "        donosnow = flux[dopeakswe:nd].SWE.idxmin() # pull the day of now snow\n",
      "\n",
      "        ablationseason = (donosnow-dopeakswe).days # compute the ablation season length\n",
      "\n",
      "        data.loc[wy,'gsET'] = flux[dopeakswe:nd].ET.sum() # sum ET over the growing season \n",
      "        data.loc[wy,'ablationslope'] = data.loc[wy,'peakswe']/ablationseason # compute the average daily melt rate and store it\n",
      "        data.loc[wy,'ablationseason'] = ablationseason # save the ablation season length\n",
      "        data.loc[wy,'ablationseasonP'] = forc[dopeakswe:donosnow].P.sum()/ablationseason\n",
      "\n",
      "        data.loc[wy,'wyP'] = forc[strt:nd].P.sum() # sum and store water year P\n",
      "    \n",
      "    data['gsET_P']= data.gsET/data.wyP # compute partitioning\n",
      "    data['flux'] = data.ablationslope+data.ablationseasonP # compute total water flux\n",
      "    \n",
      "    data.loc[data.gsET_P>1,:] = np.NaN # do a little QA/QC\n",
      "    \n",
      "    ## regress the data\n",
      "    data2 = data.dropna(subset=['flux','gsET_P']).astype(float)\n",
      "    X = data2.flux\n",
      "    \n",
      "    if X.empty:\n",
      "        return lat,lon,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN\n",
      "    else:\n",
      "        y = data2.gsET_P\n",
      "        X = sm.add_constant(X)\n",
      "        results = sm.OLS(y, X).fit()\n",
      "\n",
      "        # return the following:\n",
      "        # latitude, longitude, slope, intercept, r2, pvalue, adjusted r2\n",
      "\n",
      "        slope = results.params[1]\n",
      "        intercept = results.params[0]\n",
      "\n",
      "        return lat,lon,slope,intercept,results.rsquared,results.f_pvalue, results.rsquared_adj\n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load the files\n",
      "files = pd.read_pickle('./forcing_fluxes_filenames.dataframe')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "files.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>exists</th>\n",
        "      <th>flux</th>\n",
        "      <th>forcing</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> True</td>\n",
        "      <td> /Users/barnhatb/research/vic/fluxes/ascii/lati...</td>\n",
        "      <td> /Users/barnhatb/research/vic/forcing/ascii/lat...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> True</td>\n",
        "      <td> /Users/barnhatb/research/vic/fluxes/ascii/lati...</td>\n",
        "      <td> /Users/barnhatb/research/vic/forcing/ascii/lat...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> True</td>\n",
        "      <td> /Users/barnhatb/research/vic/fluxes/ascii/lati...</td>\n",
        "      <td> /Users/barnhatb/research/vic/forcing/ascii/lat...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> True</td>\n",
        "      <td> /Users/barnhatb/research/vic/fluxes/ascii/lati...</td>\n",
        "      <td> /Users/barnhatb/research/vic/forcing/ascii/lat...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> True</td>\n",
        "      <td> /Users/barnhatb/research/vic/fluxes/ascii/lati...</td>\n",
        "      <td> /Users/barnhatb/research/vic/forcing/ascii/lat...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "  exists                                               flux  \\\n",
        "0   True  /Users/barnhatb/research/vic/fluxes/ascii/lati...   \n",
        "1   True  /Users/barnhatb/research/vic/fluxes/ascii/lati...   \n",
        "2   True  /Users/barnhatb/research/vic/fluxes/ascii/lati...   \n",
        "3   True  /Users/barnhatb/research/vic/fluxes/ascii/lati...   \n",
        "4   True  /Users/barnhatb/research/vic/fluxes/ascii/lati...   \n",
        "\n",
        "                                             forcing  \n",
        "0  /Users/barnhatb/research/vic/forcing/ascii/lat...  \n",
        "1  /Users/barnhatb/research/vic/forcing/ascii/lat...  \n",
        "2  /Users/barnhatb/research/vic/forcing/ascii/lat...  \n",
        "3  /Users/barnhatb/research/vic/forcing/ascii/lat...  \n",
        "4  /Users/barnhatb/research/vic/forcing/ascii/lat...  "
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "files.flux[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "'/Users/barnhatb/research/vic/fluxes/ascii/latitude.25.03125/flux_snow_25.03125_-100.03125'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ProgressBar as pb\n",
      "prog = pb.ProgressBar(len(files))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "prog = pb.ProgressBar(len(files))\n",
      "\n",
      "res = []\n",
      "for i in xrange(0,len(files)):\n",
      "    fl = files.iloc[i,:].flux\n",
      "    fr = files.iloc[i,:].forcing\n",
      "    \n",
      "    lat = float(fl.split('_')[-2]) # extract latitude from file name\n",
      "    lon = float(fl.split('_')[-1]) # extract longitude from file name\n",
      "     \n",
      "    # preallocate a matrix for the results\n",
      "    data = pd.DataFrame(columns=cols, index=wyears)\n",
      "    \n",
      "    # load the data\n",
      "    flux = pd.read_table(fl, sep='\\t', names=fluxes_columns) # load fluxes\n",
      "    flux.index = pd.DatetimeIndex(dates)\n",
      "    \n",
      "    forc = pd.read_table(fr, sep=' ', names=forcing_columns) # load forcing\n",
      "    forc.index = pd.DatetimeIndex(dates)\n",
      "    \n",
      "    for wy in wyears:\n",
      "        strt = str(wy-1)+'-10-01'\n",
      "        nd = str(wy)+'-09-30'\n",
      "\n",
      "        data.loc[wy,'peakswe'] = flux[strt:nd].SWE.max() # pull max swe\n",
      "        dopeakswe = flux[strt:nd].SWE.idxmax() # pull day of peak swe\n",
      "        donosnow = flux[dopeakswe:nd].SWE.idxmin() # pull the day of now snow\n",
      "\n",
      "        ablationseason = (donosnow-dopeakswe).days # compute the ablation season length\n",
      "\n",
      "        data.loc[wy,'gsET'] = flux[dopeakswe:nd].ET.sum() # sum ET over the growing season \n",
      "        data.loc[wy,'ablationslope'] = data.loc[wy,'peakswe']/ablationseason # compute the average daily melt rate and store it\n",
      "        data.loc[wy,'ablationseason'] = ablationseason # save the ablation season length\n",
      "        data.loc[wy,'ablationseasonP'] = forc[dopeakswe:donosnow].P.sum()/ablationseason\n",
      "\n",
      "        data.loc[wy,'wyP'] = forc[strt:nd].P.sum() # sum and store water year P\n",
      "    \n",
      "    data['gsET_P']= data.gsET/data.wyP # compute partitioning\n",
      "    data['flux'] = data.ablationslope+data.ablationseasonP # compute total water flux\n",
      "    \n",
      "    data.loc[data.gsET_P>1,:] = np.NaN # do a little QA/QC\n",
      "    \n",
      "    ## regress the data\n",
      "    data2 = data.dropna(subset=['flux','gsET_P']).astype(float)\n",
      "    X = data2.flux\n",
      "    y = data2.gsET_P\n",
      "    X = sm.add_constant(X)\n",
      "    \n",
      "    if X.empty:\n",
      "        res.append((lat,lon,np.NaN,np.NaN,np.NaN,np.NaN,np.NaN))\n",
      "        continue\n",
      "    \n",
      "    results = sm.OLS(y, X).fit()\n",
      "    \n",
      "    # return the following:\n",
      "    # latitude, longitude, slope, intercept, r2, pvalue, adjusted r2\n",
      "    \n",
      "    slope = results.params[1]\n",
      "    intercept = results.params[0]\n",
      "    \n",
      "    res.append((lat,lon,slope,intercept,results.rsquared,results.f_pvalue,results.rsquared_adj))\n",
      "    \n",
      "    prog.animate(i)\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = view.map(partitioning,files.flux,files.forcing)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "309673"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(files)\n",
      "print len(res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "309673\n",
        "309673\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lat,lon,slope,intercept,rsquared,pvalue,rsquared_adj = zip(*res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lat = np.array(lat)\n",
      "lon = np.array(lon)\n",
      "slope = np.array(slope)\n",
      "intercept = np.array(intercept)\n",
      "rsquared = np.array(rsquared)\n",
      "pvalue = np.array(pvalue)\n",
      "rsquared_adj = np.array(rsquared_adj)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.savez_compressed('./whole_domain_partitioning_results.npz',lat=lat,lon=lon,slope=slope,intercept=intercept,rsquared=rsquared,pvalue=pvalue,rsquared_adj=rsquared_adj)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "lat = np.reshape(lat,[1,len(lat)])"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "lat.shape"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "lon"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# generate a longitude grid\n",
      "lon2 = np.sort(lon)\n",
      "lons = np.repeat(lon2,len(lat),axis=0)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "lons.shape"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "print np.nanmax(slope)\n",
      "print np.nanmin(slope)"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "from mpl_toolkits.basemap import Basemap, shiftgrid, cm"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "fig = plt.figure(figsize=(8,8))\n",
      "ax = fig.add_axes([0.1,0.1,0.8,0.8])\n",
      "m = Basemap(width=1000000, height= 1500000, projection='aea', lat_0 = 38, lon_0 = -108,\n",
      "              resolution = 'l', area_thresh = 1000.)\n",
      "m.drawcoastlines()\n",
      "m.drawstates()\n",
      "m.drawcountries()\n",
      "\n",
      "# draw parallels\n",
      "parallels = np.arange(0.,90,10.)\n",
      "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
      "# draw meridians\n",
      "meridians = np.arange(180.,360.,10.)\n",
      "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
      "\n",
      "# project coordinates\n",
      "x,y= m(lon,lat)\n",
      "\n",
      "clevs = list(np.arange(-0.4,1.,.1))\n",
      "cs = m.contourf(x,y,slope,clevs)\n",
      "\n",
      "#m.scatter(x,y,s=2,c=slope,alpha=0.8,marker='s', edgecolor='none')"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "slope"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "plt.pcolormesh(lon,lat,slope)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}